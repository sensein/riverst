{
  "title": "Dynamic Testing",
  "description": "Schema for configuring the dynamic test activity.",
  "type": "object",
  "properties": {
    "name": {
      "type": "string",
      "const": "dynamic-test"
    },
    "description": {
      "type": "string",
      "const": "Dynamic Transitions Test"
    },
    "options": {
      "type": "object",
      "properties": {
        "advanced_flows": {
          "type": "boolean",
          "const": true,
          "default": true,
          "description": "This activity requires advanced flows."
        },
        "advanced_flows_config_path": {
          "type": "string",
          "default": "./assets/activities/flows/dynamic-test.json",
          "const": "./assets/activities/flows/dynamic-test.json",
          "description": "Path to the advanced flows configuration file."
        },
        "pipeline_modality": {
          "type": "string",
          "enum": ["classic"],
          "default": "classic",
          "description": "The modality of the pipeline. This is set to 'classic' for the stt, llm and tts pipeline, and to 'e2e' for the end to end pipeline. 'e2e' is not supported for advanced flows."
        },
        "llm_type": {
          "type": "string",
          "enum": ["openai", "openai_realtime_beta", "gemini", "llama3.2"],
          "default": "openai",
          "description": "The LLM service to use."
        },
        "stt_type": {
          "type": "string",
          "enum": ["whisper", "openai"],
          "default": "openai",
          "description": "The STT service to use."
        },
        "tts_type": {
          "type": "string",
          "enum": ["kokoro", "elevenlabs"],
          "default": "kokoro",
          "description": "The TTS service to use."
        },
        "body_animations": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": ["dance", "wave", "i_have_a_question", "thank_you", "i_dont_know", "ok", "thumbup", "thumbdown", "happy", "sad", "angry", "fear", "disgust", "love", "sleep"]
          },
          "default": ["dance", "wave", "i_have_a_question", "thank_you", "i_dont_know", "ok", "thumbup", "thumbdown", "happy", "sad", "angry", "fear", "disgust", "love", "sleep"],
          "description": "List of body animations that the avatar can perform."
        },
        "camera_settings": {
          "type": "string",
          "enum": ["full", "mid", "upper", "head"],
          "default": "upper",
          "description": "Select the camera framing used for the avatar during the interaction."
        },
        "user_description": {
          "type": "string",
          "maxLength": 500,
          "default": "The user is a rad dude yo.",
          "description": "Optional free-text field to describe the user."
        },
        "long_term_memory": {
          "type": "boolean",
          "default": false,
          "description": "Enable or disable long-term memory. If enabled, the avatar will remember information from previous interactions and use it to guide the conversation."
        },
        "video_flag": {
          "type": "boolean",
          "default": false,
          "description": "Enable or disable video processing during the session."
        },
        "video_out_width": {
          "type": "integer",
          "const": 640,
          "default": 640,
          "description": "Width of the video output in pixels."
        },
        "video_out_height": {
          "type": "integer",
          "const": 320,
          "default": 320,
          "description": "Height of the video output in pixels."
        },
        "video_out_framerate": {
          "type": "integer",
          "const": 30,
          "default": 30,
          "description": "Framerate of the video output in frames per second."
        },
        "user_transcript": {
          "type": "boolean",
          "default": false,
          "description": "Enable showing the transcript of the user's speech. FUTURE WORK: This cannot be implemented yet because some LLM services send transcripts in the wrong order (see Gemini and OpenAI realtime beta)."
        },
        "bot_transcript": {
          "type": "boolean",
          "default": false,
          "description": "Enable showing the transcript of the avatar's speech."
        }
      },
      "required": [
        "advanced_flows",
        "advanced_flows_config_path",
        "camera_settings",
        "video_flag",
        "user_transcript",
        "bot_transcript"
      ],
      "allOf": [
        {
          "if": {
            "properties": {
              "advanced_flows": { "enum": [true] }
            }
          },
          "then": {
            "required": ["advanced_flows_config_path"]
          }
        }
      ]
    }
  },
  "required": ["name", "description", "options"]
}
