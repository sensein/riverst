{
  "title": "Vocab Tutoring",
  "description": "Schema for configuring a Vocab tutoring activity.",
  "type": "object",
  "properties": {
    "name": {
      "type": "string",
      "const": "vocab-tutoring"
    },
    "description": {
      "type": "string",
      "const": "Vocab Tutoring"
    },
    "options": {
      "type": "object",
      "properties": {
        "advanced_flows": {
          "type": "boolean",
          "const": true,
          "default": true,
          "description": "This activity requires advanced flows for structured KIVA interactions."
        },
        "advanced_flows_config_path": {
          "type": "string",
          "default": "./assets/activities/flows/vocab-tutoring.json",
          "const": "./assets/activities/flows/vocab-tutoring.json",
          "description": "Path to the advanced flows configuration file."
        },
        "session_variables_path": {
          "type": "string",
          "title": "Books",
          "dynamicEnum": "books",
          "default": "./assets/books/the_tale_of_peter_rabbit/paginated_story.json",
          "description": "Select the book to use for vocabulary tutoring."
        },
        "pipeline_modality": {
          "type": "string",
          "enum": ["classic"],
          "default": "classic",
          "description": "The modality of the pipeline. This is set to 'classic' for the stt, llm and tts pipeline, and to 'e2e' for the end to end pipeline. 'e2e' is not supported for advanced flows."
        },
        "llm_type": {
          "type": "string",
          "enum": ["openai", "openai_realtime_beta", "gemini", "llama3.2"],
          "default": "openai",
          "description": "The LLM service to use."
        },
        "stt_type": {
          "type": "string",
          "enum": ["whisper", "openai"],
          "default": "openai",
          "description": "The STT service to use."
        },
        "tts_type": {
          "type": "string",
          "enum": ["openai", "piper"],
          "default": "openai",
          "description": "The TTS service to use."
        },
        "body_animations": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": ["wave", "dance", "i_dont_know"]
          },
          "default": ["wave", "dance", "i_dont_know"],
          "description": "List of body animations that the avatar can perform."
        },
        "camera_settings": {
          "type": "string",
          "enum": ["half_body", "headshot", "full_body"],
          "default": "headshot",
          "description": "Select the camera framing used for the avatar during the interaction."
        },
        "user_description": {
          "type": "string",
          "maxLength": 500,
          "default": "The user is a 4-grade student. They want to learn vocabulary words from a book.",
          "description": "Optional free-text field to describe the user."
        },
        "long_term_memory": {
          "type": "boolean",
          "default": false,
          "description": "Enable or disable long-term memory. If enabled, the avatar will remember information from previous interactions and use it to guide the conversation."
        },
        "video_flag": {
          "type": "boolean",
          "default": false,
          "description": "Enable or disable video processing during the session."
        },
        "video_out_width": {
          "type": "integer",
          "const": 640,
          "default": 640,
          "description": "Width of the video output in pixels."
        },
        "video_out_height": {
          "type": "integer",
          "const": 320,
          "default": 320,
          "description": "Height of the video output in pixels."
        },
        "video_out_framerate": {
          "type": "integer",
          "const": 30,
          "default": 30,
          "description": "Framerate of the video output in frames per second."
        },
        "user_transcript": {
          "type": "boolean",
          "default": true,
          "description": "Enable showing the transcript of the user's speech. FUTURE WORK: This cannot be implemented yet because some LLM services send transcripts in the wrong order (see Gemini and OpenAI realtime beta)."
        },
        "bot_transcript": {
          "type": "boolean",
          "default": true,
          "description": "Enable showing the transcript of the avatar's speech."
        }
      },
      "required": [
        "advanced_flows",
        "advanced_flows_config_path",
        "camera_settings",
        "video_flag",
        "user_transcript",
        "bot_transcript"
      ],
      "allOf": [
        {
          "if": {
            "properties": {
              "advanced_flows": { "enum": [true] }
            }
          },
          "then": {
            "required": ["advanced_flows_config_path"]
          }
        }
      ]
    }
  },
  "required": ["name", "description", "options"]
}
